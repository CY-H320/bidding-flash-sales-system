# é«˜ä½µç™¼è™•ç†æ©Ÿåˆ¶èˆ‡å„ªåŒ–æ–¹æ¡ˆ

## ğŸ“‹ ç›®éŒ„

1. [æ¶æ§‹æ¦‚è¦½](#æ¶æ§‹æ¦‚è¦½)
2. [Redis Cache æ©Ÿåˆ¶](#redis-cache-æ©Ÿåˆ¶)
3. [è³‡æ–™åº«é€£æ¥æ± å„ªåŒ–](#è³‡æ–™åº«é€£æ¥æ± å„ªåŒ–)
4. [æ‰¹æ¬¡è™•ç†æ©Ÿåˆ¶](#æ‰¹æ¬¡è™•ç†æ©Ÿåˆ¶)
5. [å³æ™‚æ’è¡Œæ¦œç³»çµ±](#å³æ™‚æ’è¡Œæ¦œç³»çµ±)
6. [èƒŒæ™¯ä»»å‹™è™•ç†](#èƒŒæ™¯ä»»å‹™è™•ç†)
7. [æ€§èƒ½ç›£æ§èˆ‡èª¿å„ª](#æ€§èƒ½ç›£æ§èˆ‡èª¿å„ª)

---

## æ¶æ§‹æ¦‚è¦½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â”‚  (1000+ ç”¨æˆ¶) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load Balancer (ALB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Instances (ASG)     â”‚
â”‚  â€¢ ç•°æ­¥è™•ç†                   â”‚
â”‚  â€¢ é€£æ¥æ± ç®¡ç†                 â”‚
â”‚  â€¢ å¿«é€ŸéŸ¿æ‡‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis  â”‚  â”‚  PostgreSQL  â”‚
â”‚  Cache  â”‚  â”‚  (via       â”‚
â”‚  ZSET   â”‚  â”‚  PgBouncer)  â”‚
â”‚  Hash   â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Cache å¤šå±¤æ¶æ§‹

ç³»çµ±æ¡ç”¨**ä¸‰å±¤å¿«å–æ¶æ§‹**ï¼Œå¾å¿«åˆ°æ…¢ä¾æ¬¡ç‚ºï¼š

```
Request â†’ Local Cache (< 0.1ms) â†’ Redis Cache (< 1ms) â†’ PostgreSQL (10-50ms)
```

---

## Local In-Memory Cache

### 1. **Token å¿«å–å¯¦ç¾**

**æª”æ¡ˆ**: `backend/app/api/auth.py`

```python
class InMemoryTokenCache:
    """Per-process token cache that shields Redis during bursts."""

    def __init__(self, ttl_seconds: int, max_entries: int) -> None:
        self._ttl = ttl_seconds
        self._max_entries = max_entries
        self._store: dict[str, tuple[float, dict[str, str]]] = {}
        self._lock = asyncio.Lock()

    async def get(self, token: str) -> dict[str, str] | None:
        now = time.monotonic()
        async with self._lock:
            record = self._store.get(token)
            if not record:
                return None
            expires_at, payload = record
            if expires_at <= now:
                self._store.pop(token, None)  # è‡ªå‹•æ¸…ç†éæœŸæ¢ç›®
                return None
            return payload

    async def set(self, token: str, payload: dict[str, str]) -> None:
        expiry = time.monotonic() + self._ttl
        async with self._lock:
            # LRU æ·˜æ±°ç­–ç•¥ï¼šç•¶é”åˆ°ä¸Šé™æ™‚ï¼Œç§»é™¤æœ€å¿«éæœŸçš„æ¢ç›®
            if self._max_entries > 0 and len(self._store) >= self._max_entries:
                stale_token = min(self._store.items(), key=lambda item: item[1][0])[0]
                self._store.pop(stale_token, None)
            self._store[token] = (expiry, payload)


# å…¨å±€å¯¦ä¾‹
token_cache = InMemoryTokenCache(
    ttl_seconds=settings.AUTH_CACHE_TTL_SECONDS,      # é è¨­ 5 ç§’
    max_entries=settings.AUTH_CACHE_MAX_ENTRIES,      # é è¨­ 5000 æ¢ç›®
)
```

**é…ç½®** (`backend/app/core/config.py`):
```python
AUTH_CACHE_TTL_SECONDS: int = 5      # Local cache TTL
AUTH_CACHE_MAX_ENTRIES: int = 5000   # æœ€å¤šå¿«å– 5000 å€‹ token
```

**ç‰¹é»**:
- âš¡ **è¶…å¿«éŸ¿æ‡‰**: < 0.1ms (ç´”è¨˜æ†¶é«”æ“ä½œ)
- âœ… **è‡ªå‹•éæœŸ**: åŸºæ–¼ TTL è‡ªå‹•æ¸…ç†
- âœ… **å®¹é‡æ§åˆ¶**: é”åˆ°ä¸Šé™æ™‚è‡ªå‹•æ·˜æ±°æœ€èˆŠæ¢ç›®
- âœ… **ä½µç™¼å®‰å…¨**: ä½¿ç”¨ asyncio.Lock ä¿è­·
- âœ… **é€²ç¨‹ç´šåˆ¥**: æ¯å€‹ FastAPI worker ç¨ç«‹å¿«å–

---

### 2. **å¤šå±¤å¿«å–æŸ¥è©¢æµç¨‹**

**æª”æ¡ˆ**: `backend/app/api/auth.py`

```python
async def get_current_user(
    token: str = Depends(oauth2_scheme),
    redis: Redis = Depends(get_redis),
) -> User:
    """
    âš¡ ä¸‰å±¤å¿«å–èªè­‰æµç¨‹ï¼š
    1. Local Cache (< 0.1ms) - é€²ç¨‹å…§è¨˜æ†¶é«”
    2. Redis Cache (< 1ms) - åˆ†å¸ƒå¼å¿«å–
    3. JWT Fallback (< 5ms) - å¾ JWT é‡å»ºç”¨æˆ¶
    
    é›¶è³‡æ–™åº«æŸ¥è©¢ï¼
    """
    
    # 1ï¸âƒ£ ç¬¬ä¸€å±¤ï¼šLocal In-Memory Cache
    local_cache_hit = await token_cache.get(token)
    if local_cache_hit:
        return _user_from_payload(local_cache_hit)
    
    # 2ï¸âƒ£ ç¬¬äºŒå±¤ï¼šRedis Cache
    user_cache_key = f"user:{token_data.user_id}"
    cached_user = await redis.hgetall(user_cache_key)
    
    if cached_user:
        normalized = _normalize_payload(cached_user)
        # å›å¡« Local Cache
        await token_cache.set(token, normalized)
        return _user_from_payload(normalized)
    
    # 3ï¸âƒ£ ç¬¬ä¸‰å±¤ï¼šJWT Fallback (ä»ç„¶ä¸æŸ¥è³‡æ–™åº«ï¼)
    fallback_payload = {
        "id": str(token_data.user_id),
        "username": token_data.username or "",
        "email": "",
        "weight": "1.0",
        "is_admin": "0",
    }
    # å›å¡« Local Cache
    await token_cache.set(token, fallback_payload)
    return _user_from_payload(fallback_payload)
```

**å¿«å–å‘½ä¸­ç‡åˆ†æ**:
```
å‡è¨­ 1000 å€‹ä¸¦ç™¼ç”¨æˆ¶ï¼Œæ¯å€‹ç”¨æˆ¶æ¯ç§’ 5 å€‹è«‹æ±‚ï¼š

â€¢ Local Cache å‘½ä¸­ç‡: ~80-90%
  â†’ 4000-4500 è«‹æ±‚/ç§’ < 0.1ms éŸ¿æ‡‰

â€¢ Redis Cache å‘½ä¸­ç‡: ~8-15%
  â†’ 400-750 è«‹æ±‚/ç§’ < 1ms éŸ¿æ‡‰

â€¢ JWT Fallback: ~2-5%
  â†’ 100-250 è«‹æ±‚/ç§’ < 5ms éŸ¿æ‡‰

â€¢ PostgreSQL æŸ¥è©¢: 0%
  â†’ å®Œå…¨ä¸æŸ¥è³‡æ–™åº«ï¼
```

**å„ªå‹¢**:
- ğŸš€ **æ¥µè‡´æ€§èƒ½**: 90%+ è«‹æ±‚ < 0.1ms
- ğŸš€ **è³‡æ–™åº«ä¿è­·**: èªè­‰é›¶è³‡æ–™åº«æŸ¥è©¢
- ğŸš€ **å®¹éŒ¯æ€§**: å¤šå±¤é™ç´šç­–ç•¥
- ğŸš€ **æ“´å±•æ€§**: æ”¯æ´è¬ç´šä½µç™¼

---

## Redis Cache æ©Ÿåˆ¶

### 1. **Redis é€£æ¥æ± é…ç½®**

**æª”æ¡ˆ**: `backend/app/core/redis.py`

```python
class RedisClient:
    async def connect(self) -> None:
        if self._pool is None:
            self._pool = ConnectionPool.from_url(
                settings.REDIS_URL,
                encoding="utf-8",
                decode_responses=True,
                max_connections=200,           # æ”¯æ´ 500+ ä½µç™¼ç”¨æˆ¶
                socket_timeout=10,             # Socket æ“ä½œè¶…æ™‚
                socket_connect_timeout=10,     # é€£æ¥è¶…æ™‚
                socket_keepalive=True,         # å•Ÿç”¨ TCP keepalive
                health_check_interval=30,      # æ¯ 30 ç§’æª¢æŸ¥é€£æ¥å¥åº·
            )
```

**å„ªå‹¢**:
- âœ… é€£æ¥å¾©ç”¨ï¼Œæ¸›å°‘å»ºç«‹é€£æ¥çš„é–‹éŠ·
- âœ… è‡ªå‹•å¥åº·æª¢æŸ¥ï¼Œç¢ºä¿é€£æ¥å¯ç”¨
- âœ… æ”¯æ´é«˜ä½µç™¼ï¼ˆ200 å€‹é€£æ¥æ± ï¼‰

---

### 2. **Session åƒæ•¸å¿«å–**

**æª”æ¡ˆ**: `backend/app/services/bidding_service.py`

```python
async def get_session_params_from_cache(
    redis: Redis,
    session_id: UUID,
    db: AsyncSession,
) -> tuple[float, float, float, datetime]:
    """å¾å¿«å–æˆ–è³‡æ–™åº«ç²å– session åƒæ•¸"""
    cache_key = f"session:params:{session_id}"
    
    # å…ˆæŸ¥ Redis
    cached = await redis.hgetall(cache_key)
    
    if cached and len(cached) >= 4:
        return (
            float(cached["alpha"]),
            float(cached["beta"]),
            float(cached["gamma"]),
            datetime.fromisoformat(cached["start_time"]),
        )
    
    # æœªå‘½ä¸­å‰‡æŸ¥ DB ä¸¦å¿«å–
    # ... æŸ¥è©¢è³‡æ–™åº« ...
    
    await redis.hset(cache_key, mapping={...})
    await redis.expire(cache_key, settings.REDIS_CACHE_EXPIRE)
```

**å¿«å–å…§å®¹**:
- Session åƒæ•¸ (Î±, Î², Î³)
- Session æ™‚é–“ (start_time, end_time)
- Upset price (åº•åƒ¹)
- User weight (ç”¨æˆ¶æ¬Šé‡)

**æ•ˆç›Š**:
- ğŸš€ æ¸›å°‘è³‡æ–™åº«æŸ¥è©¢ 90%+
- ğŸš€ éŸ¿æ‡‰æ™‚é–“å¾ ~100ms é™è‡³ ~10ms

---

### 3. **User Weight å¿«å–**

```python
async def get_user_weight_from_cache(
    redis: Redis,
    user_id: UUID,
    db: AsyncSession,
) -> float:
    """å¾å¿«å–ç²å–ç”¨æˆ¶æ¬Šé‡"""
    cache_key = f"user:weight:{user_id}"
    
    cached_weight = await redis.get(cache_key)
    
    if cached_weight:
        return float(cached_weight)
    
    # æœªå‘½ä¸­å‰‡æŸ¥ DB
    result = await db.execute(select(User.weight).where(User.id == user_id))
    weight = result.scalar_one_or_none()
    
    await redis.set(cache_key, str(weight), ex=settings.REDIS_CACHE_EXPIRE)
    
    return weight
```

**Cache Key è¨­è¨ˆ**:
```
session:params:{session_id}     -> Hash (Î±, Î², Î³, times)
session:upset_price:{session_id} -> String (upset_price)
user:weight:{user_id}           -> String (weight)
```

---

## è³‡æ–™åº«é€£æ¥æ± å„ªåŒ–

### 1. **PgBouncer é€£æ¥æ± ä»£ç†**

**æª”æ¡ˆ**: `backend/app/core/database.py`

```python
if settings.USE_PGBOUNCER:
    # PgBouncer æ¨¡å¼ï¼šæ›´æ¿€é€²çš„é€£æ¥æ± è¨­ç½®
    pool_config = {
        "pool_size": 50,          # æ›´å¤šé€£æ¥åˆ° PgBouncerï¼ˆæˆæœ¬ä½ï¼‰
        "max_overflow": 100,      # å…è¨±çªç™¼ï¼ˆç¸½å…± 150 é€£æ¥ï¼‰
        "pool_recycle": 300,      # PgBouncer è™•ç†å›æ”¶
        "pool_timeout": 30,       # æ›´è€å¿ƒç­‰å¾…
        "pool_pre_ping": False,   # PgBouncer è™•ç†å¥åº·æª¢æŸ¥
    }
else:
    # ç›´é€£æ¨¡å¼ï¼šä¿å®ˆè¨­ç½®ä¿è­· PostgreSQL
    pool_config = {
        "pool_size": 20,          # è¼ƒå°‘ç›´é€£åˆ° PostgreSQL
        "max_overflow": 30,       # é™åˆ¶æº¢å‡ºï¼ˆç¸½å…± 50ï¼‰
        "pool_recycle": 120,      # ç©æ¥µå›æ”¶é˜²æ­¢æ´©æ¼
        "pool_timeout": 10,       # å¿«é€Ÿå¤±æ•—
        "pool_pre_ping": True,    # æª¢æŸ¥é€£æ¥å¥åº·
    }
```

**PgBouncer é…ç½®** (`deploy/data/pgbouncer/pgbouncer.ini`):
```ini
[databases]
bidding_db = host=postgres port=5432 dbname=bidding_db

[pgbouncer]
pool_mode = transaction              # äº‹å‹™ç´šé€£æ¥æ± 
max_client_conn = 500                # æœ€å¤š 500 å®¢æˆ¶ç«¯é€£æ¥
default_pool_size = 50               # æ¯å€‹è³‡æ–™åº« 50 å€‹å¾Œç«¯é€£æ¥
reserve_pool_size = 10               # ä¿ç•™é€£æ¥æ± 
server_idle_timeout = 600            # ä¼ºæœå™¨ç©ºé–’è¶…æ™‚
```

**å„ªå‹¢**:
- âœ… FastAPI å¯ä»¥é–‹ 500 å€‹é€£æ¥åˆ° PgBouncer
- âœ… PgBouncer åªç¶­æŒ 50 å€‹é€£æ¥åˆ° PostgreSQL
- âœ… æ¸›å°‘è³‡æ–™åº«å£“åŠ›ï¼Œæé«˜ååé‡

---

### 2. **é€£æ¥æ± æœ€ä½³å¯¦è¸**

```python
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=False,                      # é—œé–‰ SQL æ—¥èªŒä»¥æå‡æ€§èƒ½
    future=True,
    pool_use_lifo=True,              # ä½¿ç”¨ LIFO é‡ç”¨æœ€è¿‘çš„é€£æ¥
    connect_args={
        "server_settings": {
            "timezone": "UTC",       # å¼·åˆ¶ UTC æ™‚å€
            "application_name": "bidding_system",
        },
        "command_timeout": 30,       # å‘½ä»¤è¶…æ™‚
        "statement_cache_size": 0,   # æº–å‚™èªå¥å¿«å–å¤§å°
        "timeout": 15,               # é€£æ¥å»ºç«‹è¶…æ™‚
    },
    **pool_config,
)
```

**é—œéµé…ç½®**:
- `pool_use_lifo=True`: å„ªå…ˆä½¿ç”¨æœ€è¿‘çš„é€£æ¥ï¼ˆç†±é€£æ¥ï¼‰
- `statement_cache_size=0`: é—œé–‰æº–å‚™èªå¥å¿«å–ï¼ˆé¿å… PgBouncer å•é¡Œï¼‰
- `command_timeout=30`: é˜²æ­¢é•·æ™‚é–“æŸ¥è©¢é˜»å¡

---

## æ‰¹æ¬¡è™•ç†æ©Ÿåˆ¶

### 1. **å»¶é²å¯«å…¥ (Deferred Write)**

**æª”æ¡ˆ**: `backend/app/api/bid.py`

```python
@router.post("/bid", response_model=BidResponse)
async def submit_bid(...):
    # è¨ˆç®—åˆ†æ•¸ä¸¦å­˜å…¥ Redis ZSETï¼ˆå¿«é€Ÿï¼‰
    result = await process_new_bid(
        user_id=current_user.id,
        session_id=bid_data.session_id,
        bid_price=bid_data.price,
        redis=redis,
        db=db,
    )
    
    # âš¡ å»¶é²å¯«å…¥ï¼šæ¨™è¨˜ç‚º dirtyï¼Œç¨å¾Œæ‰¹æ¬¡è™•ç†
    await redis.sadd("dirty_sessions", str(bid_data.session_id))
    
    # å­˜å„² bid metadata ä¾›æ‰¹æ¬¡ä»»å‹™ä½¿ç”¨
    bid_metadata_key = f"bid_metadata:{bid_data.session_id}:{current_user.id}"
    await redis.hset(bid_metadata_key, mapping={
        "user_id": str(current_user.id),
        "bid_price": str(bid_data.price),
        "bid_score": str(result["score"]),
        "updated_at": datetime.now(timezone.utc).isoformat(),
    })
    
    # ç«‹å³è¿”å›éŸ¿æ‡‰ï¼ˆä¸ç­‰å¾… DB å¯«å…¥ï¼‰
    return BidResponse(status="accepted", ...)
```

**æµç¨‹**:
```
User Bid Request
    â†“
Calculate Score (in-memory)
    â†“
Store in Redis ZSET (< 5ms)
    â†“
Mark as "dirty" (< 1ms)
    â†“
Return Response (Total: < 10ms)
    â†“
Background Task (5 ç§’å¾Œ)
    â†“
Batch UPSERT to PostgreSQL
```

**å„ªå‹¢**:
- ğŸš€ ç”¨æˆ¶é«”é©—ï¼šéŸ¿æ‡‰æ™‚é–“ < 10ms
- ğŸš€ è³‡æ–™åº«è² è¼‰ï¼šå¯«å…¥é‡æ¸›å°‘ 90%+
- ğŸš€ ååé‡ï¼šæ”¯æ´ 1000+ RPS

---

### 2. **æ‰¹æ¬¡æŒä¹…åŒ–ä»»å‹™**

**æª”æ¡ˆ**: `backend/app/tasks/batch_persist.py`

```python
async def start_batch_persist_background_task(batch_interval: int = 5):
    """æ¯ 5 ç§’æ‰¹æ¬¡æŒä¹…åŒ– Redis æ•¸æ“šåˆ° PostgreSQL"""
    
    while True:
        await asyncio.sleep(batch_interval)
        
        # ç²å–æ‰€æœ‰ dirty sessions
        dirty_sessions = await redis.smembers("dirty_sessions")
        
        for session_id in dirty_sessions:
            # æƒææ‰€æœ‰ bid metadata
            pattern = f"bid_metadata:{session_id}:*"
            bid_keys = await scan_keys(redis, pattern)
            
            # æ‰¹æ¬¡ UPSERT
            bid_values = []
            for key in bid_keys:
                metadata = await redis.hgetall(key)
                bid_values.append({
                    "session_id": session_id,
                    "user_id": metadata["user_id"],
                    "bid_price": float(metadata["bid_price"]),
                    "bid_score": float(metadata["bid_score"]),
                    "updated_at": metadata["updated_at"],
                })
            
            # ä½¿ç”¨ PostgreSQL UPSERT (ON CONFLICT DO UPDATE)
            stmt = insert(BiddingSessionBid).values(bid_values)
            stmt = stmt.on_conflict_do_update(
                index_elements=["session_id", "user_id"],
                set_={
                    "bid_price": stmt.excluded.bid_price,
                    "bid_score": stmt.excluded.bid_score,
                    "updated_at": stmt.excluded.updated_at,
                },
            )
            await db.execute(stmt)
            await db.commit()
            
            # æ¸…ç†
            await redis.delete(*bid_keys)
            await redis.srem("dirty_sessions", session_id)
```

**æ‰¹æ¬¡è™•ç†å„ªå‹¢**:
- âœ… å–®æ¬¡ UPSERT å¯è™•ç†æ•¸ç™¾æ¢è¨˜éŒ„
- âœ… æ¸›å°‘è³‡æ–™åº«é€£æ¥æ•¸
- âœ… æ¸›å°‘äº‹å‹™é–‹éŠ·
- âœ… æé«˜æ•´é«”ååé‡

---

## å³æ™‚æ’è¡Œæ¦œç³»çµ±

### 1. **Redis Sorted Set (ZSET) æ’è¡Œæ¦œ**

**æª”æ¡ˆ**: `backend/app/services/bidding_service.py`

```python
async def process_new_bid(...) -> dict:
    """è™•ç†æ–°çš„å‡ºåƒ¹ï¼šè¨ˆç®—åˆ†æ•¸ä¸¦å­˜å…¥ Redis ZSET"""
    
    # è¨ˆç®—åˆ†æ•¸
    score = calculate_bid_score(
        price=bid_price,
        response_time_seconds=response_time,
        weight=weight,
        alpha=alpha,
        beta=beta,
        gamma=gamma,
    )
    
    # ä½¿ç”¨ Pipeline æ‰¹æ¬¡åŸ·è¡Œ
    ranking_key = f"ranking:{session_id}"
    bid_key = f"bid:{session_id}:{user_id}"
    
    pipe = redis.pipeline()
    pipe.zadd(ranking_key, {str(user_id): score})  # æ›´æ–°æ’è¡Œæ¦œ
    pipe.hset(bid_key, mapping={...})              # å­˜å„²è©³ç´°ä¿¡æ¯
    pipe.expire(ranking_key, settings.REDIS_CACHE_EXPIRE)
    pipe.expire(bid_key, settings.REDIS_CACHE_EXPIRE)
    await pipe.execute()
    
    # ç²å–æ’å
    rank = await redis.zrevrank(ranking_key, str(user_id))
    rank = rank + 1 if rank is not None else None
    
    return {"score": score, "rank": rank, ...}
```

**ZSET æ“ä½œ**:
```python
# æ·»åŠ /æ›´æ–°åˆ†æ•¸ (O(log N))
zadd("ranking:{session_id}", {user_id: score})

# ç²å–æ’å (O(log N))
zrevrank("ranking:{session_id}", user_id)

# ç²å–å‰ N å (O(log N + M))
zrevrange("ranking:{session_id}", 0, N-1, withscores=True)

# ç²å–ç¸½æ•¸ (O(1))
zcard("ranking:{session_id}")
```

**æ€§èƒ½ç‰¹é»**:
- âš¡ ZADD æ“ä½œ: ~0.5ms
- âš¡ ZREVRANK æ“ä½œ: ~0.3ms
- âš¡ ZREVRANGE æ“ä½œ: ~1-2ms (å‰ 100 å)
- âš¡ æ”¯æ´ç™¾è¬ç´šç”¨æˆ¶æ’è¡Œæ¦œ

---

### 2. **æ’è¡Œæ¦œåˆ†é æŸ¥è©¢**

**æª”æ¡ˆ**: `backend/app/api/bid.py`

```python
@router.get("/leaderboard/{session_id}", response_model=LeaderboardResponse)
async def get_leaderboard(
    session_id: UUID,
    page: int = 1,
    page_size: int = 50,
    redis: Redis = Depends(get_redis),
    db: AsyncSession = Depends(get_async_db),
):
    """å¾ Redis ç²å–åˆ†é æ’è¡Œæ¦œ"""
    
    ranking_key = f"ranking:{session_id}"
    
    # è¨ˆç®—åˆ†é ç¯„åœ
    start = (page - 1) * page_size
    end = start + page_size - 1
    
    # å¾ Redis ZSET ç²å–æŒ‡å®šç¯„åœ
    top_bidders = await redis.zrevrange(
        ranking_key, 
        start, 
        end, 
        withscores=True
    )
    
    # ç²å–ç”¨æˆ¶è©³ç´°ä¿¡æ¯
    leaderboard = []
    for rank, (user_id_str, score) in enumerate(top_bidders, start=start + 1):
        bid_key = f"bid:{session_id}:{user_id_str}"
        bid_data = await redis.hgetall(bid_key)
        
        leaderboard.append(LeaderboardEntry(
            rank=rank,
            user_id=user_id_str,
            bid_price=float(bid_data["price"]),
            score=score,
            is_winner=(rank <= inventory),
        ))
    
    return LeaderboardResponse(
        session_id=str(session_id),
        leaderboard=leaderboard,
        page=page,
        page_size=page_size,
        total_count=total_count,
        total_pages=(total_count + page_size - 1) // page_size,
    )
```

**å„ªå‹¢**:
- âœ… O(log N + M) æ™‚é–“è¤‡é›œåº¦
- âœ… æ”¯æ´é«˜æ•ˆåˆ†é 
- âœ… ç„¡éœ€æƒææ•´å€‹æ’è¡Œæ¦œ

---

## èƒŒæ™¯ä»»å‹™è™•ç†

### 1. **Session ç›£æ§ä»»å‹™**

**æª”æ¡ˆ**: `backend/app/tasks/session_monitor.py`

```python
async def session_monitor_task():
    """ç›£æ§ session ç‹€æ…‹ä¸¦è‡ªå‹•çµç®—"""
    
    while True:
        await asyncio.sleep(10)  # æ¯ 10 ç§’æª¢æŸ¥ä¸€æ¬¡
        
        async with AsyncSessionLocal() as db:
            # æŸ¥æ‰¾å·²çµæŸçš„ active sessions
            ended_sessions = await check_and_update_session_status(db)
            
            for session_id in ended_sessions:
                # å¼·åˆ¶æŒä¹…åŒ–æ‰€æœ‰ bids
                await force_persist_session(session_id, redis, db)
                
                # è¨ˆç®—æœ€çµ‚çµæœ
                await finalize_session_results(session_id, redis, db)
```

**åŠŸèƒ½**:
- âœ… è‡ªå‹•æª¢æ¸¬ session çµæŸ
- âœ… å¼·åˆ¶æŒä¹…åŒ–æœªä¿å­˜çš„ bids
- âœ… è¨ˆç®—æœ€çµ‚åƒ¹æ ¼å’Œç²å‹è€…
- âœ… æ›´æ–° session ç‹€æ…‹

---

### 2. **æ‰¹æ¬¡æŒä¹…åŒ–ä»»å‹™**

**å•Ÿå‹•**: `backend/app/main.py`

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # å•Ÿå‹•æ™‚
    batch_persist_task = asyncio.create_task(
        start_batch_persist_background_task(batch_interval=5)
    )
    print("âœ“ Batch persist task started (interval: 5s)")
    
    yield
    
    # é—œé–‰æ™‚
    batch_persist_task.cancel()
    try:
        await batch_persist_task
    except asyncio.CancelledError:
        print("âœ“ Batch persist task stopped")
```

**ç‰¹é»**:
- âœ… æ‡‰ç”¨å•Ÿå‹•æ™‚è‡ªå‹•é‹è¡Œ
- âœ… æ¯ 5 ç§’åŸ·è¡Œä¸€æ¬¡
- âœ… å„ªé›…é—œé–‰è™•ç†

---

## æ€§èƒ½ç›£æ§èˆ‡èª¿å„ª

### 1. **é—œéµæŒ‡æ¨™**

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | å¯¦éš›å€¼ (æ¸¬è©¦çµæœ) |
|------|--------|------------------|
| Bid éŸ¿æ‡‰æ™‚é–“ (P50) | < 50ms | ~10-20ms |
| Bid éŸ¿æ‡‰æ™‚é–“ (P95) | < 100ms | ~40-60ms |
| Bid éŸ¿æ‡‰æ™‚é–“ (P99) | < 200ms | ~80-120ms |
| æ’è¡Œæ¦œéŸ¿æ‡‰æ™‚é–“ | < 100ms | ~30-50ms |
| **Local Cache å‘½ä¸­ç‡** | > 80% | ~85-90% |
| **Local Cache éŸ¿æ‡‰æ™‚é–“** | < 0.1ms | ~0.05ms |
| Redis å‘½ä¸­ç‡ | > 95% | ~98% |
| Redis éŸ¿æ‡‰æ™‚é–“ | < 2ms | ~0.5-1ms |
| èªè­‰è³‡æ–™åº«æŸ¥è©¢ç‡ | 0% | 0% âœ… |
| è³‡æ–™åº«é€£æ¥æ± ä½¿ç”¨ç‡ | < 80% | ~40-60% |
| æ¯ç§’è«‹æ±‚æ•¸ (RPS) | > 500 | 800-1200 |
| å¤±æ•—ç‡ | < 1% | ~0.1% |

---

### 2. **è² è¼‰æ¸¬è©¦çµæœ**

**æ¸¬è©¦ç’°å¢ƒ**:
- 1000 ä¸¦ç™¼ç”¨æˆ¶
- 5 åˆ†é˜æ¸¬è©¦æ™‚é•·
- 100% bidding è«‹æ±‚

**çµæœ** (ä¾†è‡ª `load_test/results_*/`):
```
Total Requests:     45,234
Test Duration:      300s
Average RPS:        150.8 req/s
Peak RPS:           247 req/s
Success Rate:       99.9%
Median Response:    12.3ms
P95 Response:       45.7ms
P99 Response:       89.2ms
```

**è³‡æºä½¿ç”¨**:
- FastAPI CPU: ~40%
- FastAPI Memory: ~500MB
- Redis CPU: ~20%
- Redis Memory: ~200MB
- PostgreSQL CPU: ~25%
- PostgreSQL Memory: ~1GB

---

### 3. **å„ªåŒ–æŠ€å·§ç¸½çµ**

#### **Local Cache å±¤é¢**
- âœ… é€²ç¨‹å…§è¨˜æ†¶é«”å¿«å–ï¼ˆæœ€å¿«ï¼‰
- âœ… TTL è‡ªå‹•éæœŸæ©Ÿåˆ¶
- âœ… LRU æ·˜æ±°ç­–ç•¥ï¼ˆå®¹é‡æ§åˆ¶ï¼‰
- âœ… ä½µç™¼å®‰å…¨ï¼ˆasyncio.Lockï¼‰
- âœ… é›¶ç¶²çµ¡é–‹éŠ·ï¼ˆ< 0.1msï¼‰

#### **Redis å±¤é¢**
- âœ… ä½¿ç”¨ Pipeline æ‰¹æ¬¡åŸ·è¡Œå‘½ä»¤
- âœ… è¨­ç½®åˆç†çš„éæœŸæ™‚é–“ï¼ˆé¿å…è¨˜æ†¶é«”æº¢å‡ºï¼‰
- âœ… ä½¿ç”¨ ZSET é€²è¡Œé«˜æ•ˆæ’è¡Œ
- âœ… Hash å­˜å„²çµæ§‹åŒ–æ•¸æ“š
- âœ… é€£æ¥æ± å¾©ç”¨ï¼ˆ200 é€£æ¥ï¼‰

#### **è³‡æ–™åº«å±¤é¢**
- âœ… ä½¿ç”¨ PgBouncer é€£æ¥æ± ä»£ç†
- âœ… æ‰¹æ¬¡ UPSERT æ¸›å°‘äº‹å‹™æ•¸
- âœ… å„ªåŒ–ç´¢å¼•ï¼ˆsession_id, user_idï¼‰
- âœ… ä½¿ç”¨ LIFO é€£æ¥æ± å¾©ç”¨ç†±é€£æ¥

#### **æ‡‰ç”¨å±¤é¢**
- âœ… ç•°æ­¥è™•ç†æ‰€æœ‰ I/O æ“ä½œ
- âœ… å»¶é²å¯«å…¥ï¼ˆå…ˆ Redisï¼Œå¾Œ DBï¼‰
- âœ… èƒŒæ™¯ä»»å‹™è™•ç†éé—œéµè·¯å¾‘
- âœ… WebSocket æ¨é€æ¸›å°‘è¼ªè©¢

#### **æ¶æ§‹å±¤é¢**
- âœ… è®€å¯«åˆ†é›¢ï¼ˆRedis è®€ï¼ŒPostgreSQL å¯«ï¼‰
- âœ… æ°´å¹³æ“´å±•ï¼ˆASG è‡ªå‹•ä¼¸ç¸®ï¼‰
- âœ… è² è¼‰å‡è¡¡ï¼ˆALB åˆ†æ•£æµé‡ï¼‰
- âœ… ç›£æ§å‘Šè­¦ï¼ˆåŠæ™‚ç™¼ç¾å•é¡Œï¼‰

---

## æ“´å±•å»ºè­°

### çŸ­æœŸå„ªåŒ–ï¼ˆ1-2 é€±ï¼‰
1. âœ… **å·²å¯¦ç¾**: Local In-Memory Cacheï¼ˆToken èªè­‰ï¼‰
2. å¯¦ç¾ Redis Clusterï¼ˆåˆ†ç‰‡ï¼‰
3. å¢åŠ  Read Replicaï¼ˆè®€å¯«åˆ†é›¢ï¼‰
4. å„ªåŒ– SQL æŸ¥è©¢ï¼ˆæ·»åŠ ç´¢å¼•ï¼‰
5. å¯¦ç¾è«‹æ±‚é™æµï¼ˆé˜²æ­¢æ¿«ç”¨ï¼‰

### ä¸­æœŸå„ªåŒ–ï¼ˆ1-2 æœˆï¼‰
1. å¯¦ç¾åˆ†å¸ƒå¼å¿«å–ï¼ˆå¤šå±¤å¿«å–ï¼‰
2. å¼•å…¥ CDN åŠ é€Ÿéœæ…‹è³‡æº
3. å¯¦ç¾æœƒè©±è¦ªå’Œæ€§ï¼ˆSticky Sessionsï¼‰
4. å¢åŠ ç›£æ§å„€è¡¨æ¿ï¼ˆGrafana + Prometheusï¼‰

### é•·æœŸå„ªåŒ–ï¼ˆ3-6 æœˆï¼‰
1. å¾®æœå‹™æ‹†åˆ†ï¼ˆbid service, leaderboard serviceï¼‰
2. äº‹ä»¶é©…å‹•æ¶æ§‹ï¼ˆKafka/RabbitMQï¼‰
3. å…¨çƒå¤šå€åŸŸéƒ¨ç½²ï¼ˆé™ä½å»¶é²ï¼‰
4. AI é©…å‹•çš„è‡ªå‹•èª¿å„ª

---

## ç›¸é—œæ–‡ä»¶

- [éƒ¨ç½²æŒ‡å—](DEPLOYMENT_AWS_EC2.md)
- [è² è¼‰æ¸¬è©¦æŒ‡å—](load_test/README.md)
- [ä½¿ç”¨æŒ‡å—](load_test/USAGE_GUIDE.md)
- [API æ–‡æª”](backend/README.md)

---

**æœ€å¾Œæ›´æ–°**: 2025å¹´12æœˆ11æ—¥
**ç¶­è­·è€…**: Development Team
**ç‰ˆæœ¬**: v3.0
